{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show ALL outputs in cell, not only last result\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_filepath = \"../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set relative path mapping for module imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(relative_filepath)\n",
    "\n",
    "#for path in sys.path:\n",
    "#    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pickled combined data\n",
    "X_y_data = pd.read_pickle(relative_filepath + \"data/interim/step_4/X_y_data.pkl\")\n",
    "\n",
    "# Read in pickled train data\n",
    "X_y_train = pd.read_pickle(relative_filepath + \"data/interim/step_4/X_y_train.pkl\")\n",
    "\n",
    "# Read in pickled test data\n",
    "X_y_test = pd.read_pickle(relative_filepath + \"data/interim/step_4/X_y_test.pkl\")\n",
    "\n",
    "# Recap data structure\n",
    "X_y_data.head()\n",
    "X_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.22</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>14.96</td>\n",
       "      <td>19.10</td>\n",
       "      <td>97.03</td>\n",
       "      <td>687.3</td>\n",
       "      <td>0.08992</td>\n",
       "      <td>0.09823</td>\n",
       "      <td>0.05940</td>\n",
       "      <td>0.048190</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.05852</td>\n",
       "      <td>...</td>\n",
       "      <td>26.19</td>\n",
       "      <td>109.10</td>\n",
       "      <td>809.8</td>\n",
       "      <td>0.1313</td>\n",
       "      <td>0.30300</td>\n",
       "      <td>0.18040</td>\n",
       "      <td>0.14890</td>\n",
       "      <td>0.2962</td>\n",
       "      <td>0.08472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.68</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.07395</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>...</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.1464</td>\n",
       "      <td>0.18710</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>10.32</td>\n",
       "      <td>16.35</td>\n",
       "      <td>65.31</td>\n",
       "      <td>324.9</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.01012</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06201</td>\n",
       "      <td>...</td>\n",
       "      <td>21.77</td>\n",
       "      <td>71.12</td>\n",
       "      <td>384.9</td>\n",
       "      <td>0.1285</td>\n",
       "      <td>0.08842</td>\n",
       "      <td>0.04384</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.2681</td>\n",
       "      <td>0.07399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>11.85</td>\n",
       "      <td>17.46</td>\n",
       "      <td>75.54</td>\n",
       "      <td>432.7</td>\n",
       "      <td>0.08372</td>\n",
       "      <td>0.05642</td>\n",
       "      <td>0.02688</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.05715</td>\n",
       "      <td>...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>84.35</td>\n",
       "      <td>517.8</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.17580</td>\n",
       "      <td>0.13160</td>\n",
       "      <td>0.09140</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>0.07007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "562        15.22         30.62          103.40      716.9          0.10480   \n",
       "291        14.96         19.10           97.03      687.3          0.08992   \n",
       "16         14.68         20.13           94.74      684.5          0.09867   \n",
       "546        10.32         16.35           65.31      324.9          0.09434   \n",
       "293        11.85         17.46           75.54      432.7          0.08372   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "562           0.20870         0.25500             0.094290         0.2128   \n",
       "291           0.09823         0.05940             0.048190         0.1879   \n",
       "16            0.07200         0.07395             0.052590         0.1586   \n",
       "546           0.04994         0.01012             0.005495         0.1885   \n",
       "293           0.05642         0.02688             0.022800         0.1875   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "562                 0.07152  ...          42.79           128.70       915.0   \n",
       "291                 0.05852  ...          26.19           109.10       809.8   \n",
       "16                  0.05922  ...          30.88           123.40      1138.0   \n",
       "546                 0.06201  ...          21.77            71.12       384.9   \n",
       "293                 0.05715  ...          25.75            84.35       517.8   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "562            0.1417            0.79170          1.17000   \n",
       "291            0.1313            0.30300          0.18040   \n",
       "16             0.1464            0.18710          0.29140   \n",
       "546            0.1285            0.08842          0.04384   \n",
       "293            0.1369            0.17580          0.13160   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "562               0.23560          0.4089                  0.14090       0  \n",
       "291               0.14890          0.2962                  0.08472       1  \n",
       "16                0.16090          0.3029                  0.08216       0  \n",
       "546               0.02381          0.2681                  0.07399       1  \n",
       "293               0.09140          0.3101                  0.07007       1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(426, 31)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#values for config dict\n",
    "input_dfs = [X_y_data,\n",
    "             X_y_train,\n",
    "             X_y_test]\n",
    "\n",
    "target = \"classLabel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://contrib.scikit-learn.org/category_encoders/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://feature-engine.readthedocs.io/en/latest/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/the-art-of-finding-the-best-features-for-machine-learning-a9074e2ca60d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.scikit-yb.org/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indicator Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://elitedatascience.com/feature-engineering-best-practices\n",
    "    \n",
    "What is Not Feature Engineering?\n",
    "That means there are certain steps we do not consider to be feature engineering:\n",
    "\n",
    "We do not consider initial data collection to be feature engineering.\n",
    "Similarly, we do not consider creating the target variable to be feature engineering.\n",
    "We do not consider removing duplicates, handling missing values, or fixing mislabeled classes to be feature engineering. We put these under data cleaning.\n",
    "We do not consider scaling or normalization to be feature engineering because these steps belong inside the cross-validation loop (i.e. after you’ve already built your analytical base table).\n",
    "Finally, we do not consider feature selection or PCA to be feature engineering. These steps also belong inside your cross-validation loop.\n",
    "Again, this is simply our categorization. Reasonable data scientists may disagree, and that’s perfectly fine.\n",
    "\n",
    "With those disclaimers out of the way, let’s dive into the best practices and heuristics!\n",
    "\n",
    "Indicator Variables\n",
    "    Indicator variable from thresholds\n",
    "    Indicator variable from multiple features\n",
    "    Indicator variable for special events\n",
    "    Indicator variable for groups of classes\n",
    "    \n",
    "Interaction Features\n",
    "    Sum of two features\n",
    "    Difference between two features\n",
    "    Product of two features\n",
    "    Quotient of two features\n",
    "    \n",
    "Feature Representation\n",
    "    Date and time features\n",
    "    Numeric to categorical mappings\n",
    "    Grouping sparse classes\n",
    "    Creating dummy variables\n",
    "\n",
    "External Data\n",
    "    Time series data\n",
    "    External API’s\n",
    "    Geocoding\n",
    "    Other sources of the same data\n",
    "    \n",
    "Error Analysis (Post-Modeling)\n",
    "    Start with larger errors\n",
    "    Segment by classes\n",
    "    Unsupervised clustering\n",
    "    Ask colleagues or domain experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/feature-engineering-in-python-part-i-the-most-powerful-way-of-dealing-with-data-8e2447e7c69e\n",
    "    \n",
    "Feature Selection\n",
    "\n",
    "Feature Transformation\n",
    "\n",
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Creating New Features\n",
    "This is the hardest section and requires the most critical thinking, in my opinion. \n",
    "There isn’t any code I can give that will apply to lots of projects — it really depends on the dataset.\n",
    "Here are a couple of cases where you may want to try creating a new feature:\n",
    "You suspect that the relationship of an outcome and a feature depends on a second feature → Create an interaction variable\n",
    "You want to create linear relationships → Create quadratic or higher level functions\n",
    "You can think of variables/information that is missing from your dataset → Create this variable using a function of variables you do have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance (An estimate of the usefulness of a feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features are allocated scores and can then be ranked by their scores. \n",
    "This can be helpful as a pre-cursor to selecting features\n",
    "A feature may be important if it is highly correlated with the dependent variable \n",
    "More complex predictive modeling algorithms perform feature importance and selection internally while constructing their model.\n",
    "Some examples include MARS, Random Forest and Gradient Boosted Machines. \n",
    "These models can also report on the variable importance determined during the model preparation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05226303 0.01614678 0.04063694 0.0485314  0.00868061 0.01966329\n",
      " 0.07190355 0.05143262 0.00806197 0.0060422  0.0170291  0.00489626\n",
      " 0.02268818 0.04587105 0.00449967 0.00556324 0.01228311 0.00678393\n",
      " 0.0052182  0.00441903 0.08776628 0.02606867 0.10190227 0.07822523\n",
      " 0.02076684 0.02670533 0.03407161 0.14919924 0.01333855 0.0093418 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# fit an Extra Trees model to the data\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# display the relative importance of each attribute\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection (From many features to a subset that are useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Those attributes that are irrelevant to the problem need to be removed. \n",
    "There will be some features that will be more important than others to the model accuracy. \n",
    "There will also be features that will be redundant in the context of other features.\n",
    "Feature selection algorithms may use correlation or other feature importance methods.\n",
    "\n",
    "Regularization methods like LASSO and ridge regression may also be considered algorithms with feature selection baked in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction (The automatic construction of new features from raw data: Eg PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Process of automatically reducing the dimensionality into a much smaller set that can be modelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Construction (The manual construction of new features from raw data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature importance and selection can inform you about the objective utility of features, but those features have to come from somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iterative Process of Feature Engineering\n",
    "The process might look as follows:\n",
    "\n",
    "Brainstorm features: Really get into the problem, look at a lot of data, study feature engineering on other problems and see what you can steal.\n",
    "Devise features: Depends on your problem, but you may use automatic feature extraction, manual feature construction and mixtures of the two.\n",
    "Select features: Use different feature importance scorings and feature selection methods to prepare one or more “views” for your models to operate upon.\n",
    "Evaluate models: Estimate model accuracy on unseen data using the chosen features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://adataanalyst.com/machine-learning/comprehensive-guide-feature-engineering/\n",
    "\n",
    "Representing timestamps\n",
    "Missing Value Treatment\n",
    "Create new ratios and proportions\n",
    "Binning/Bucketing\n",
    "Reframe Numerical Quantities\n",
    "Variable Transformation\n",
    "Outlier Detection and Treatment\n",
    "Decomposing Categorical Attributes\n",
    "Encode all the categorical features in Python\n",
    "One Hot Encoding in Python\n",
    "Feature Crosses\n",
    "Feature Importance:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
